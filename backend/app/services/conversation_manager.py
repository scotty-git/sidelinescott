"""
ConversationManager - Core stateful conversation processing with CleanerContext

This is the heart of the CleanerContext system - manages stateful conversation 
cleaning using cleaned history in sliding window context.
"""

import time
import logging
import asyncio
from typing import Dict, List, Optional, Any
from uuid import UUID
from sqlalchemy.orm import Session
from sqlalchemy import desc

from app.models.conversation import Conversation
from app.models.turn import Turn
from app.core.database import get_db
from app.services.gemini_service import GeminiService
from app.services.prompt_engineering_service import PromptEngineeringService
from app.core.variables import SUPPORTED_VARIABLES

logger = logging.getLogger(__name__)

class ConversationState:
    """Manages the stateful context for a single conversation"""
    
    def __init__(self, conversation_id: UUID, sliding_window_size: int = 10):
        self.conversation_id = conversation_id
        self.sliding_window_size = sliding_window_size  # Configurable sliding window
        self.cleaned_history: List[Dict[str, Any]] = []
        self.context_patterns: Dict[str, Any] = {}
        
        print(f"[ConversationState] Initialized for conversation {conversation_id}")
        print(f"[ConversationState] Sliding window size: {self.sliding_window_size}")
    
    def get_cleaned_sliding_window(self) -> List[Dict[str, Any]]:
        """Get the cleaned conversation history for context (NOT raw text)"""
        print(f"[ConversationState] üîç SLIDING WINDOW DEBUG: Getting sliding window")
        print(f"[ConversationState] üìä SLIDING WINDOW DEBUG: Current history length: {len(self.cleaned_history)}")
        print(f"[ConversationState] üéØ SLIDING WINDOW DEBUG: Window size configured: {self.sliding_window_size}")
        
        # Debug: Log ALL history before windowing
        if len(self.cleaned_history) > 0:
            print(f"[ConversationState] üìã SLIDING WINDOW DEBUG: All available history:")
            for i, turn in enumerate(self.cleaned_history):
                print(f"[ConversationState]   History[{i}]: {turn['speaker']} - '{turn['cleaned_text'][:60]}...'")
        
        # Return last N turns of CLEANED conversation history
        window = self.cleaned_history[-self.sliding_window_size:]
        
        print(f"[ConversationState] ‚úÇÔ∏è SLIDING WINDOW DEBUG: Sliding window contains {len(window)} turns")
        print(f"[ConversationState] üéØ SLIDING WINDOW DEBUG: Window range: turns {max(0, len(self.cleaned_history) - self.sliding_window_size)} to {len(self.cleaned_history)}")
        
        if len(window) > 0:
            print(f"[ConversationState] üìã SLIDING WINDOW DEBUG: Window contents:")
            for i, turn in enumerate(window):
                actual_turn_num = len(self.cleaned_history) - len(window) + i + 1
                print(f"[ConversationState]   Window[{i}] (Turn {actual_turn_num}): {turn['speaker']} -> '{turn['cleaned_text'][:80]}...'")
        else:
            print(f"[ConversationState] ‚ùå SLIDING WINDOW DEBUG: Window is empty!")
        
        return window
    
    def add_to_history(self, turn_data: Dict[str, Any]):
        """Add a processed turn to the cleaned history"""
        print(f"[ConversationState] Adding turn to history: {turn_data['speaker']}")
        print(f"[ConversationState] Raw text: '{turn_data['raw_text'][:100]}...'")
        print(f"[ConversationState] Cleaned text: '{turn_data['cleaned_text'][:100]}...'")
        
        self.cleaned_history.append(turn_data)
        
        print(f"[ConversationState] History now contains {len(self.cleaned_history)} turns")
    
    def update_context_patterns(self, patterns: Dict[str, Any]):
        """Track patterns detected in this conversation"""
        print(f"[ConversationState] Updating context patterns: {patterns}")
        self.context_patterns.update(patterns)


class ConversationManager:
    """
    Core stateful conversation management with CleanerContext processing.
    
    Key Innovation: Uses CLEANED history in sliding window, not raw STT errors.
    This creates self-improving conversation context.
    """
    
    def __init__(self):
        self.active_conversations: Dict[UUID, ConversationState] = {}
        self.gemini_service = GeminiService()
        self.prompt_service = PromptEngineeringService()
        self.performance_metrics: Dict[str, List[float]] = {
            'lumen_processing_times': [],
            'user_processing_times': [],
            'context_retrieval_times': []
        }
        
        print("[ConversationManager] Initialized with stateful conversation tracking and Gemini 2.5 Flash")
        print("[ConversationManager] Performance metrics tracking enabled")
        print("[ConversationManager] Prompt Engineering Service integrated")
    
    def get_conversation_state(self, conversation_id: UUID, sliding_window_size: int = 10, db: Session = None) -> ConversationState:
        """Get or create conversation state for stateful processing"""
        print(f"[ConversationManager] Getting conversation state for {conversation_id}")
        
        if conversation_id not in self.active_conversations:
            print(f"[ConversationManager] Creating new conversation state for {conversation_id}")
            self.active_conversations[conversation_id] = ConversationState(conversation_id, sliding_window_size)
            
            # Load existing turns from database to rebuild context
            self._load_existing_context(conversation_id, db)
        else:
            print(f"[ConversationManager] Using existing conversation state for {conversation_id}")
            # Update sliding window size if provided
            if sliding_window_size != 10:  # Only update if different from default
                self.active_conversations[conversation_id].sliding_window_size = sliding_window_size
                print(f"[ConversationManager] Updated sliding window size to {sliding_window_size}")
        
        return self.active_conversations[conversation_id]
    
    def _load_existing_context(self, conversation_id: UUID, db: Session = None):
        """Load existing turns from database to rebuild conversation context"""
        print(f"[ConversationManager] üîç CONTEXT DEBUG: Loading existing context for conversation {conversation_id}")
        
        if not db:
            print(f"[ConversationManager] ‚ùå CONTEXT DEBUG: No database session available - starting with fresh context")
            return
        
        try:
            # Query existing turns for this conversation, ordered chronologically
            print(f"[ConversationManager] üîç CONTEXT DEBUG: Querying database for existing turns...")
            existing_turns = db.query(Turn).filter(
                Turn.conversation_id == conversation_id
            ).order_by(Turn.created_at.asc()).all()
            
            print(f"[ConversationManager] üìä CONTEXT DEBUG: Found {len(existing_turns)} existing turns in database")
            
            if not existing_turns:
                print(f"[ConversationManager] ‚ùå CONTEXT DEBUG: No existing turns found - starting with fresh context")
                return
            
            # Get the conversation state for this conversation
            conversation_state = self.active_conversations.get(conversation_id)
            if not conversation_state:
                print(f"[ConversationManager] ‚ùå CONTEXT DEBUG: No conversation state found for {conversation_id}")
                return
            
            print(f"[ConversationManager] üîç CONTEXT DEBUG: Starting to load {len(existing_turns)} turns into context...")
            
            # Convert database turns to conversation state format
            for i, turn in enumerate(existing_turns):
                turn_data = {
                    'conversation_id': conversation_id,
                    'speaker': turn.speaker,
                    'raw_text': turn.raw_text,
                    'cleaned_text': turn.cleaned_text,
                    'confidence_score': turn.confidence_score,
                    'cleaning_applied': turn.cleaning_applied,
                    'cleaning_level': turn.cleaning_level,
                    'processing_time_ms': turn.processing_time_ms,
                    'corrections': turn.corrections or [],
                    'context_detected': turn.context_detected,
                    'ai_model_used': turn.ai_model_used
                }
                
                # Add to conversation state history
                conversation_state.cleaned_history.append(turn_data)
                
                # Enhanced logging for each turn
                print(f"[ConversationManager] üìù CONTEXT DEBUG: Loaded Turn {i+1}: {turn.speaker}")
                print(f"[ConversationManager]   ‚Ü≥ Raw: '{turn.raw_text[:60]}...'")
                print(f"[ConversationManager]   ‚Ü≥ Cleaned: '{turn.cleaned_text[:60]}...'")
                print(f"[ConversationManager]   ‚Ü≥ Added to history (total: {len(conversation_state.cleaned_history)})")
            
            print(f"[ConversationManager] ‚úÖ CONTEXT DEBUG: Successfully loaded {len(existing_turns)} turns into conversation context")
            print(f"[ConversationManager] üìà CONTEXT DEBUG: Total history length: {len(conversation_state.cleaned_history)}")
            print(f"[ConversationManager] üéØ CONTEXT DEBUG: Context spans from turn 1 to turn {len(existing_turns)}")
            
            # Log ALL turns for debugging
            if len(conversation_state.cleaned_history) > 0:
                print(f"[ConversationManager] üìã CONTEXT DEBUG: Complete context history:")
                for i, turn in enumerate(conversation_state.cleaned_history):
                    print(f"[ConversationManager]   Turn {i+1}: {turn['speaker']} - '{turn['cleaned_text'][:80]}...'")
                    
        except Exception as e:
            print(f"[ConversationManager] ‚ùå CONTEXT DEBUG: Failed to load existing context: {e}")
            print(f"[ConversationManager] ‚ùå CONTEXT DEBUG: Exception type: {type(e).__name__}")
            print(f"[ConversationManager] ‚ùå CONTEXT DEBUG: Continuing with fresh context")
            import traceback
            traceback.print_exc()
    
    def _build_template_variables(self, raw_text: str, cleaned_context: List[Dict[str, Any]], 
                                 cleaning_decision: str, conversation) -> Dict[str, str]:
        """Build variables for template using pure 5-variable system - NO defaults"""
        
        # Build context string from cleaned history
        context_str = ""
        if cleaned_context:
            print(f"[ConversationManager] üìä CONTEXT BUILD: Using {len(cleaned_context)} turns for context")
            context_lines = []
            for turn in cleaned_context[-5:]:  # Last 5 turns
                line = f"{turn['speaker']}: {turn['cleaned_text']}"
                context_lines.append(line)
                print(f"[ConversationManager]   ‚Üí Context line: {line[:100]}...")
            context_str = "\n".join(context_lines)
            print(f"[ConversationManager] ‚úÖ CONTEXT BUILD: Built context string ({len(context_str)} chars)")
        else:
            print(f"[ConversationManager] ‚ùå CONTEXT BUILD: No context available!")
        
        # Build pure 5-variable system (NO defaults, NO additions)
        variables = {
            # Required variables (always present)
            "raw_text": raw_text,
            "conversation_context": context_str,  # Empty string if no context
            "cleaning_level": cleaning_decision,
            
            # Optional variables (EXACTLY as provided, or empty string)
            "call_context": conversation.call_context or "",
            "additional_context": conversation.additional_context or ""
        }
        
        return variables
    
    async def add_turn(self, conversation_id: UUID, speaker: str, raw_text: str, db: Session, 
                       sliding_window_size: int = 10, cleaning_level: str = "full", 
                       model_params: Dict[str, Any] = None, skip_transcription_errors: bool = True) -> Dict[str, Any]:
        """
        Core method: Add a turn to the conversation with stateful cleaning.
        
        This is where the CleanerContext magic happens:
        1. Skip Lumen turns (they're perfect) 
        2. For user turns, use cleaned history as context
        3. Apply intelligent cleaning based on context
        """
        start_time = time.time()
        print(f"\n[ConversationManager] ===== PROCESSING NEW TURN =====")
        print(f"[ConversationManager] Conversation: {conversation_id}")
        print(f"[ConversationManager] Speaker: {speaker}")
        print(f"[ConversationManager] Raw text: '{raw_text}'")
        
        conversation_state = self.get_conversation_state(conversation_id, sliding_window_size, db)
        
        # Check for transcription errors before processing
        if skip_transcription_errors and self._is_likely_transcription_error(raw_text):
            print(f"[ConversationManager] üö´ TRANSCRIPTION ERROR DETECTED - Skipping processing")
            print(f"[ConversationManager] Raw text flagged as error: '{raw_text}'")
            result = await self._process_transcription_error(conversation_id, speaker, raw_text, conversation_state, db)
        elif self._is_lumen_turn(speaker):
            print(f"[ConversationManager] üöÄ LUMEN TURN DETECTED - Using instant bypass")
            result = await self._process_lumen_turn(conversation_id, speaker, raw_text, conversation_state, db)
        else:
            print(f"[ConversationManager] üë§ USER TURN DETECTED - Using full CleanerContext processing")
            result = await self._process_user_turn(conversation_id, speaker, raw_text, conversation_state, db, cleaning_level, model_params)
        
        total_time = (time.time() - start_time) * 1000
        print(f"[ConversationManager] ===== TURN COMPLETE in {total_time:.2f}ms =====\n")
        
        return result
    
    def _is_lumen_turn(self, speaker: str) -> bool:
        """Detect if this is a Lumen/AI turn that should be bypassed"""
        lumen_speakers = ['Lumen', 'AI', 'Assistant', 'Claude']
        is_lumen = speaker in lumen_speakers
        
        print(f"[ConversationManager] Turn classification: {speaker} -> {'LUMEN' if is_lumen else 'USER'}")
        return is_lumen
    
    def _is_likely_transcription_error(self, text: str) -> bool:
        """Detect likely transcription errors like foreign chars, gibberish, single symbols"""
        import re
        
        # Strip whitespace for analysis
        text = text.strip()
        
        # Very short single character or symbol
        if len(text) <= 2:
            print(f"[ConversationManager] Flagged as error: too short ({len(text)} chars)")
            return True
        
        # Contains primarily non-Latin characters (Arabic, Thai, Chinese, etc.)
        # that are likely transcription errors in English conversations
        non_latin_chars = re.findall(r'[^\x00-\x7F\s]', text)
        if non_latin_chars and len(non_latin_chars) / len(text) > 0.3:
            print(f"[ConversationManager] Flagged as error: high foreign character ratio ({len(non_latin_chars)}/{len(text)})")
            return True
        
        # Single foreign characters or symbols that are clearly errors
        foreign_error_patterns = [
            r'^[ÿ£-Ÿä]+$',  # Arabic script only
            r'^[‡∏Å-‡πô]+$',  # Thai script only
            r'^[Í∞Ä-Ìû£]+$',  # Korean script only
            r'^[‰∏Ä-ÈæØ]+$',  # Chinese characters only
            r'^[–∞-—è]+$',   # Cyrillic script only
            r'^[Œ±-œâ]+$',   # Greek script only
        ]
        
        for pattern in foreign_error_patterns:
            if re.match(pattern, text):
                print(f"[ConversationManager] Flagged as error: matches foreign script pattern")
                return True
        
        return False
    
    async def _process_transcription_error(self, conversation_id: UUID, speaker: str, raw_text: str,
                                         conversation_state: ConversationState, db: Session) -> Dict[str, Any]:
        """
        Process detected transcription errors by skipping them with minimal processing.
        These are usually foreign characters or gibberish that shouldn't be processed.
        """
        process_start = time.time()
        print(f"[ConversationManager] üö´ Processing transcription error with skip")
        
        # Skip processing - mark as transcription error with empty cleaned text
        cleaned_text = ""  # Empty - indicates skipped
        processing_time_ms = 0  # Minimal processing time
        
        turn_data = {
            'conversation_id': conversation_id,
            'speaker': speaker,
            'raw_text': raw_text,
            'cleaned_text': cleaned_text,
            'confidence_score': 'LOW',  # Low confidence for transcription errors
            'cleaning_applied': True,   # We did apply "cleaning" by removing gibberish
            'cleaning_level': 'skip',
            'processing_time_ms': processing_time_ms,
            'corrections': [{
                'original': raw_text,
                'corrected': '',
                'confidence': 'HIGH',
                'reason': 'Detected as transcription error - likely foreign characters or gibberish'
            }],
            'context_detected': 'transcription_error',
            'ai_model_used': None
        }
        
        # Add to conversation history (with empty cleaned text to not pollute context)
        conversation_state.add_to_history(turn_data)
        
        # Save to database (with error handling for testing)
        try:
            db_turn = Turn(**turn_data)
            db.add(db_turn)
            db.commit()
            db.refresh(db_turn)
            
            # Ensure created_at is available for response (for testing)
            if not hasattr(db_turn, 'created_at') or db_turn.created_at is None:
                from datetime import datetime
                db_turn.created_at = datetime.utcnow()
        except Exception as e:
            print(f"[ConversationManager] ‚ö†Ô∏è Database error (continuing with mock): {e}")
            # Mock response for testing
            from datetime import datetime
            class MockTurn:
                def __init__(self):
                    self.id = "mock-transcription-error"
                    self.created_at = datetime.utcnow()
            db_turn = MockTurn()
        
        actual_processing_time = (time.time() - process_start) * 1000
        
        print(f"[ConversationManager] ‚úÖ Transcription error processed in {actual_processing_time:.2f}ms")
        print(f"[ConversationManager] Raw text skipped: '{raw_text}'")
        print(f"[ConversationManager] Cleaned text (empty): '{cleaned_text}'")
        
        return {
            'turn_id': str(db_turn.id),
            'conversation_id': str(conversation_id),
            'speaker': speaker,
            'raw_text': raw_text,
            'cleaned_text': cleaned_text,
            'metadata': {
                'confidence_score': turn_data['confidence_score'],
                'cleaning_applied': turn_data['cleaning_applied'],
                'cleaning_level': turn_data['cleaning_level'],
                'processing_time_ms': actual_processing_time,
                'corrections': turn_data['corrections'],
                'context_detected': turn_data['context_detected'],
                'ai_model_used': turn_data['ai_model_used']
            },
            'created_at': db_turn.created_at.isoformat()
        }
    
    async def _process_lumen_turn(self, conversation_id: UUID, speaker: str, raw_text: str, 
                                 conversation_state: ConversationState, db: Session) -> Dict[str, Any]:
        """
        Process Lumen turns with ZERO latency - they're already perfect.
        Target: < 10ms processing time
        """
        process_start = time.time()
        print(f"[ConversationManager] üöÄ Processing Lumen turn with instant bypass")
        
        # Lumen turns are perfect - no cleaning needed
        cleaned_text = raw_text
        processing_time_ms = 0  # Conceptually zero processing time
        
        turn_data = {
            'conversation_id': conversation_id,
            'speaker': speaker,
            'raw_text': raw_text,
            'cleaned_text': cleaned_text,
            'confidence_score': 'HIGH',
            'cleaning_applied': False,
            'cleaning_level': 'none',
            'processing_time_ms': processing_time_ms,
            'corrections': [],
            'context_detected': 'ai_response',
            'ai_model_used': None,
            'timing_breakdown': {
                'context_retrieval_ms': 0,
                'prompt_preparation_ms': 0,
                'gemini_api_ms': 0,
                'database_save_ms': 0,
                'prompt_logging_ms': 0,
                'total_ms': 0
            },
            'gemini_prompt': None,  # No prompt for Lumen turns
            'gemini_response': None  # No Gemini processing for Lumen turns
        }
        
        # Add to conversation history
        conversation_state.add_to_history(turn_data)
        
        # Save to database (with error handling for testing)
        try:
            db_turn = Turn(**turn_data)
            db.add(db_turn)
            db.commit()
            db.refresh(db_turn)
            
            # Ensure created_at is available for response (for testing)
            if not hasattr(db_turn, 'created_at') or db_turn.created_at is None:
                from datetime import datetime
                db_turn.created_at = datetime.utcnow()
        except Exception as e:
            print(f"[ConversationManager] ‚ö†Ô∏è Database error (continuing with mock): {e}")
            # Create mock turn for testing when database is unavailable
            from datetime import datetime
            import uuid
            db_turn = type('MockTurn', (), {
                'id': uuid.uuid4(),
                'created_at': datetime.utcnow()
            })()
        
        actual_processing_time = (time.time() - process_start) * 1000
        self.performance_metrics['lumen_processing_times'].append(actual_processing_time)
        
        print(f"[ConversationManager] ‚úÖ Lumen turn processed in {actual_processing_time:.2f}ms")
        print(f"[ConversationManager] Cleaning applied: {turn_data['cleaning_applied']}")
        print(f"[ConversationManager] Added to cleaned history for future context")
        
        return {
            'turn_id': str(db_turn.id),
            'conversation_id': str(conversation_id),
            'speaker': speaker,
            'raw_text': raw_text,
            'cleaned_text': cleaned_text,
            'metadata': {
                'confidence_score': turn_data['confidence_score'],
                'cleaning_applied': turn_data['cleaning_applied'],
                'cleaning_level': turn_data['cleaning_level'],
                'processing_time_ms': actual_processing_time,
                'corrections': turn_data['corrections'],
                'context_detected': turn_data['context_detected'],
                'ai_model_used': turn_data['ai_model_used']
            },
            'created_at': db_turn.created_at.isoformat()
        }
    
    async def _process_user_turn(self, conversation_id: UUID, speaker: str, raw_text: str,
                                conversation_state: ConversationState, db: Session, 
                                cleaning_level: str = "full", model_params: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Process user turns with full CleanerContext intelligence.
        Uses cleaned conversation history as context for better cleaning.
        Target: < 500ms processing time
        """
        process_start = time.time()
        print(f"[ConversationManager] üë§ Processing user turn with CleanerContext intelligence")
        
        # Initialize timing breakdown
        timing_breakdown = {
            "context_retrieval_ms": 0,
            "prompt_preparation_ms": 0,
            "gemini_api_ms": 0,
            "database_save_ms": 0,
            "prompt_logging_ms": 0,
            "total_ms": 0
        }
        
        # Get cleaned conversation context (the KEY innovation)
        print(f"[ConversationManager] üîç CONTEXT FLOW DEBUG: About to retrieve cleaned sliding window...")
        context_start = time.time()
        cleaned_context = conversation_state.get_cleaned_sliding_window()
        context_time = (time.time() - context_start) * 1000
        timing_breakdown["context_retrieval_ms"] = round(context_time, 2)
        self.performance_metrics['context_retrieval_times'].append(context_time)
        
        print(f"[ConversationManager] ‚úÖ CONTEXT FLOW DEBUG: Retrieved cleaned context in {context_time:.2f}ms")
        print(f"[ConversationManager] üìä CONTEXT FLOW DEBUG: Context contains {len(cleaned_context)} previous turns")
        
        # Debug: Log the context that will be passed to prompt building
        if len(cleaned_context) > 0:
            print(f"[ConversationManager] üìã CONTEXT FLOW DEBUG: Context being passed to prompt:")
            for i, turn in enumerate(cleaned_context):
                print(f"[ConversationManager]   Context[{i}]: {turn['speaker']} - '{turn['cleaned_text'][:80]}...'")
        else:
            print(f"[ConversationManager] ‚ùå CONTEXT FLOW DEBUG: NO CONTEXT available for prompt!")
        
        # Use provided cleaning level or analyze for decision
        if cleaning_level == "auto":
            print(f"[ConversationManager] üß† Analyzing turn for cleaning decision...")
            cleaning_decision = self._analyze_cleaning_need(raw_text)
            print(f"[ConversationManager] Decision: {cleaning_decision}")
        else:
            cleaning_decision = cleaning_level
            print(f"[ConversationManager] Using provided cleaning level: {cleaning_decision}")
        
        # Get active prompt template for processing (or use default)
        prompt_start = time.time()
        rendered_prompt_text = None
        try:
            active_template = await self.prompt_service.get_or_create_default_template(db)
            print(f"[ConversationManager] Using prompt template: {active_template.name}")
            
            # Get conversation object for context fields
            from app.models.conversation import Conversation
            conversation = db.query(Conversation).filter(
                Conversation.id == conversation_id
            ).first()
            
            if not conversation:
                raise ValueError(f"Conversation {conversation_id} not found")
            
            # Build variables using pure 5-variable system
            print(f"[ConversationManager] üéØ PURE VARIABLES: Building 5-variable system...")
            variables = self._build_template_variables(
                raw_text=raw_text,
                cleaned_context=cleaned_context,
                cleaning_decision=cleaning_decision,
                conversation=conversation
            )
            
            # Debug: Show all variables being used
            print(f"[ConversationManager] üìã PURE VARIABLES: Built {len(variables)} variables:")
            for key, value in variables.items():
                preview = value[:50] + "..." if len(value) > 50 else value
                print(f"[ConversationManager]   ‚Üí {key}: '{preview}' ({len(value)} chars)")
            
            # Validate template variables and show warnings
            validation = self.prompt_service.validate_template_variables(active_template.template, variables)
            
            # Log warnings to user (don't auto-fix)
            if validation["warnings"]:
                print(f"[ConversationManager] ‚ö†Ô∏è Template Warnings:")
                for warning in validation["warnings"]:
                    print(f"[ConversationManager]   ‚Üí {warning}")
            
            # Fail if validation errors exist
            if not validation["valid"]:
                print(f"[ConversationManager] ‚ùå Template Errors:")
                for error in validation["errors"]:
                    print(f"[ConversationManager]   ‚Üí {error}")
                raise ValueError(f"Template validation failed: {validation['errors']}")
            
            # Render the prompt with validated variables
            rendered_prompt = await self.prompt_service.render_prompt(db, active_template.id, variables)
            if rendered_prompt:
                print(f"[ConversationManager] Rendered prompt with {len(variables)} variables")
                print(f"[ConversationManager] Estimated tokens: {rendered_prompt.token_count}")
                rendered_prompt_text = rendered_prompt.rendered_prompt
            
        except Exception as e:
            print(f"[ConversationManager] ‚ö†Ô∏è Prompt service error: {e}")
            rendered_prompt = None
            active_template = None
            rendered_prompt_text = None
        
        prompt_time = (time.time() - prompt_start) * 1000
        timing_breakdown["prompt_preparation_ms"] = round(prompt_time, 2)

        # Use Gemini 2.5 Flash for actual cleaning
        print(f"[ConversationManager] ü§ñ Applying {cleaning_decision} cleaning with Gemini...")
        if model_params:
            print(f"[ConversationManager] Using custom model params: {model_params}")
        
        gemini_start = time.time()
        try:
            print(f"[ConversationManager] Calling Gemini service for {speaker} turn...")
            
            # Add progress monitoring for long API calls
            async def progress_monitor():
                await asyncio.sleep(10)  # Wait 10 seconds
                if time.time() - gemini_start > 10:
                    print(f"[ConversationManager] ‚è≥ Gemini call still running after 10s...")
                await asyncio.sleep(20)  # Wait another 20 seconds  
                if time.time() - gemini_start > 30:
                    print(f"[ConversationManager] ‚è≥ Gemini call still running after 30s...")
                await asyncio.sleep(30)  # Wait another 30 seconds
                if time.time() - gemini_start > 60:
                    print(f"[ConversationManager] ‚ö†Ô∏è Gemini call taking very long (60s+)...")
            
            # Start progress monitor
            monitor_task = asyncio.create_task(progress_monitor())
            
            # Call Gemini service
            cleaned_result = await self.gemini_service.clean_conversation_turn(
                raw_text=raw_text,
                speaker=speaker,
                cleaned_context=cleaned_context,
                cleaning_level=cleaning_decision,
                model_params=model_params,
                rendered_prompt=rendered_prompt_text  # Pass the prompt for storage
            )
            
            # Cancel progress monitor since we're done
            monitor_task.cancel()
            
            gemini_time = (time.time() - gemini_start) * 1000
            timing_breakdown["gemini_api_ms"] = round(gemini_time, 2)
            print(f"[ConversationManager] ‚úÖ Gemini processing completed in {gemini_time:.2f}ms")
            
        except Exception as e:
            gemini_time = (time.time() - gemini_start) * 1000
            timing_breakdown["gemini_api_ms"] = round(gemini_time, 2)
            
            # Enhanced logging for timeouts
            if "TimeoutError" in str(type(e).__name__):
                print(f"[ConversationManager] üö® CRITICAL TIMEOUT: Gemini API timed out after {gemini_time:.2f}ms")
                print(f"[ConversationManager] üö® TIMEOUT CONTEXT: {speaker} turn, {len(raw_text)} chars")
                print(f"[ConversationManager] üö® TEXT PREVIEW: '{raw_text[:200]}...'")
                logger.error(f"üö® CRITICAL: Gemini timeout in conversation {conversation_id}")
                logger.error(f"üö® TIMEOUT TURN: {speaker} - {len(raw_text)} chars - {raw_text[:100]}...")
            else:
                print(f"[ConversationManager] ‚ùå Gemini processing failed after {gemini_time:.2f}ms: {e}")
            
            # Create fallback result when Gemini fails
            error_type = "timeout_3s" if "TimeoutError" in str(type(e).__name__) else str(type(e).__name__)
            cleaned_result = {
                "cleaned_text": raw_text,  # Return original text as fallback
                "metadata": {
                    "confidence_score": "LOW",
                    "cleaning_applied": False,
                    "cleaning_level": "fallback",
                    "corrections": [],
                    "context_detected": f"error_fallback_{error_type}",
                    "ai_model_used": f"fallback_due_to_{error_type}",
                    "error_type": error_type,
                    "timeout_occurred": "TimeoutError" in str(type(e).__name__)
                },
                "raw_response": None
            }
            print(f"[ConversationManager] Using fallback response for {speaker} turn due to {error_type}")
        
        # Calculate final timing before creating turn_data
        processing_time_ms = (time.time() - process_start) * 1000
        self.performance_metrics['user_processing_times'].append(processing_time_ms)
        
        # Complete timing breakdown BEFORE saving to database
        # Calculate the sum of individual components to ensure math accuracy
        component_sum = (
            timing_breakdown.get("context_retrieval_ms", 0) +
            timing_breakdown.get("prompt_preparation_ms", 0) +
            timing_breakdown.get("gemini_api_ms", 0)
        )
        
        # Add placeholders (will be filled after DB operations)
        timing_breakdown["database_save_ms"] = 0  # Will be updated after DB save
        timing_breakdown["prompt_logging_ms"] = 0  # Will be updated after prompt logging
        timing_breakdown["total_ms"] = round(component_sum, 2)  # Use component sum, not wall clock time
        
        print(f"[ConversationManager] üîç TIMING FIX DEBUG: Final timing_breakdown before DB save: {timing_breakdown}")
        print(f"[ConversationManager] üìä TIMING FIX DEBUG: timing_breakdown keys: {list(timing_breakdown.keys())}")
        print(f"[ConversationManager] üìä TIMING FIX DEBUG: timing_breakdown length: {len(timing_breakdown)}")
        
        # Create a copy of timing_breakdown to avoid reference issues
        timing_breakdown_copy = timing_breakdown.copy()
        
        turn_data = {
            'conversation_id': conversation_id,
            'speaker': speaker,
            'raw_text': raw_text,
            'cleaned_text': cleaned_result['cleaned_text'],
            'confidence_score': cleaned_result['metadata']['confidence_score'],
            'cleaning_applied': str(cleaned_result['metadata']['cleaning_applied']),
            'cleaning_level': cleaned_result['metadata']['cleaning_level'],
            'processing_time_ms': processing_time_ms,
            'corrections': cleaned_result['metadata']['corrections'],
            'context_detected': cleaned_result['metadata']['context_detected'],
            'ai_model_used': cleaned_result['metadata']['ai_model_used'],
            'timing_breakdown': timing_breakdown_copy,  # Use copy to avoid reference issues
            'gemini_prompt': cleaned_result.get('prompt_used', None),
            'gemini_response': cleaned_result.get('raw_response', None)
        }
        
        # Add to conversation history (THIS IS THE KEY - cleaned version goes to history)
        conversation_state.add_to_history(turn_data)
        
        # Save to database (with error handling for testing)
        print(f"[ConversationManager] üîç DB SAVE DEBUG: Preparing to save turn to database...")
        print(f"[ConversationManager] üìä DB SAVE DEBUG: timing_breakdown data: {timing_breakdown}")
        print(f"[ConversationManager] üîç DB SAVE DEBUG: timing_breakdown type: {type(timing_breakdown)}")
        
        db_start = time.time()
        try:
            print(f"[ConversationManager] üíæ DB SAVE DEBUG: Creating Turn object with turn_data...")
            
            # Debug database configuration
            from app.core.config import settings
            print(f"[ConversationManager] üîç DB CONFIG DEBUG: DATABASE_URL: {settings.DATABASE_URL[:60]}...{settings.DATABASE_URL[-20:]}")
            print(f"[ConversationManager] üîç DB CONFIG DEBUG: Database session type: {type(db)}")
            
            # Debug the turn_data before saving
            print(f"[ConversationManager] üìã DB SAVE DEBUG: turn_data keys: {list(turn_data.keys())}")
            print(f"[ConversationManager] üìä DB SAVE DEBUG: turn_data.timing_breakdown: {turn_data.get('timing_breakdown', 'NOT_SET')}")
            print(f"[ConversationManager] üîç DB SAVE DEBUG: timing_breakdown type: {type(turn_data.get('timing_breakdown'))}")
            print(f"[ConversationManager] üìä DB SAVE DEBUG: timing_breakdown contents: {turn_data.get('timing_breakdown')}")
            
            db_turn = Turn(**turn_data)
            
            # Immediately check what got saved to the object
            print(f"[ConversationManager] üîç DB SAVE DEBUG: IMMEDIATELY after Turn creation:")
            print(f"[ConversationManager] üìä DB SAVE DEBUG: db_turn.timing_breakdown: {db_turn.timing_breakdown}")
            print(f"[ConversationManager] üìä DB SAVE DEBUG: db_turn.timing_breakdown type: {type(db_turn.timing_breakdown)}")
            print(f"[ConversationManager] üìä DB SAVE DEBUG: db_turn.timing_breakdown keys: {list(db_turn.timing_breakdown.keys()) if db_turn.timing_breakdown else 'None'}")
            
            print(f"[ConversationManager] ‚úÖ DB SAVE DEBUG: Turn object created successfully")
            print(f"[ConversationManager] üìä DB SAVE DEBUG: db_turn.timing_breakdown: {getattr(db_turn, 'timing_breakdown', 'NOT_SET')}")
            
            # Explicitly mark the timing_breakdown as modified for SQLAlchemy
            from sqlalchemy.orm.attributes import flag_modified
            flag_modified(db_turn, 'timing_breakdown')
            
            db.add(db_turn)
            db.commit()
            
            # Calculate database save time and update timing breakdown
            db_time = (time.time() - db_start) * 1000
            timing_breakdown["database_save_ms"] = round(db_time, 2)
            
            # Ensure created_at is available for response
            if not hasattr(db_turn, 'created_at') or db_turn.created_at is None:
                from datetime import datetime
                db_turn.created_at = datetime.utcnow()
                
            print(f"[ConversationManager] ‚è±Ô∏è DB SAVE DEBUG: Database save took {db_time:.2f}ms")
                
        except Exception as e:
            print(f"[ConversationManager] ‚ùå CRITICAL DB ERROR: {e}")
            print(f"[ConversationManager] ‚ùå CRITICAL ERROR TYPE: {type(e).__name__}")
            print(f"[ConversationManager] ‚ùå CRITICAL ERROR DETAILS: {str(e)}")
            
            # Print the exact turn_data that's causing the issue
            print(f"[ConversationManager] üîç PROBLEMATIC TURN DATA:")
            for key, value in turn_data.items():
                print(f"[ConversationManager]   {key}: {type(value)} = {str(value)[:100]}...")
            
            import traceback
            print(f"[ConversationManager] ‚ùå FULL TRACEBACK:")
            traceback.print_exc()
            print(f"[ConversationManager] ‚ö†Ô∏è Database error (continuing with mock): {e}")
            # Create mock turn for testing when database is unavailable
            from datetime import datetime
            import uuid
            db_turn = type('MockTurn', (), {
                'id': uuid.uuid4(),
                'created_at': datetime.utcnow(),
                'timing_breakdown': {},  # Empty timing for mock
                'processing_time_ms': 0   # Zero processing time for mock
            })()
            print(f"[ConversationManager] üîß DB SAVE DEBUG: Created mock turn for fallback")
            db_time = (time.time() - db_start) * 1000
            timing_breakdown["database_save_ms"] = round(db_time, 2)
        
        # Log prompt usage for analytics
        prompt_log_start = time.time()
        if active_template and rendered_prompt:
            try:
                await self.prompt_service.log_prompt_usage(
                    db=db,
                    template_id=active_template.id,
                    rendered_prompt=rendered_prompt.rendered_prompt,
                    variables=variables,
                    turn_id=db_turn.id,
                    conversation_id=conversation_id,
                    processing_time_ms=processing_time_ms,
                    confidence_score=turn_data['confidence_score'],
                    corrections_count=len(turn_data['corrections'])
                )
                print(f"[ConversationManager] ‚úÖ Logged prompt usage for analytics")
            except Exception as e:
                print(f"[ConversationManager] ‚ö†Ô∏è Failed to log prompt usage: {e}")
        prompt_log_time = (time.time() - prompt_log_start) * 1000
        timing_breakdown["prompt_logging_ms"] = round(prompt_log_time, 2)
        
        # Calculate accurate total from all components
        final_total = (
            timing_breakdown.get("context_retrieval_ms", 0) +
            timing_breakdown.get("prompt_preparation_ms", 0) +
            timing_breakdown.get("gemini_api_ms", 0) +
            timing_breakdown.get("database_save_ms", 0) +
            timing_breakdown.get("prompt_logging_ms", 0)
        )
        timing_breakdown["total_ms"] = round(final_total, 2)

        print(f"[ConversationManager] ‚úÖ User turn processed in {processing_time_ms:.2f}ms")
        print(f"[ConversationManager] üìä Final timing breakdown: {timing_breakdown}")
        print(f"[ConversationManager] Cleaning applied: {turn_data['cleaning_applied']}")
        print(f"[ConversationManager] Confidence: {turn_data['confidence_score']}")
        print(f"[ConversationManager] Corrections made: {len(turn_data['corrections'])}")
        
        return {
            'turn_id': str(db_turn.id),
            'conversation_id': str(conversation_id),
            'speaker': speaker,
            'raw_text': raw_text,
            'cleaned_text': turn_data['cleaned_text'],
            'metadata': {
                'confidence_score': turn_data['confidence_score'],
                'cleaning_applied': turn_data['cleaning_applied'],
                'cleaning_level': turn_data['cleaning_level'],
                'processing_time_ms': processing_time_ms,
                'timing_breakdown': timing_breakdown,
                'corrections': turn_data['corrections'],
                'context_detected': turn_data['context_detected'],
                'ai_model_used': turn_data['ai_model_used'],
                'gemini_prompt': turn_data.get('gemini_prompt', None),
                'gemini_response': turn_data.get('gemini_response', None)
            },
            'created_at': db_turn.created_at.isoformat()
        }
    
    def _analyze_cleaning_need(self, raw_text: str) -> str:
        """
        Analyze text to determine optimal cleaning level
        """
        print(f"[ConversationManager] üîç Analyzing text for cleaning patterns...")
        
        # Simple pattern detection
        text_lower = raw_text.lower()
        
        # Check for simple acknowledgments (no cleaning needed)
        simple_responses = ['yes', 'no', 'ok', 'okay', 'right', 'correct', 'exactly', 'sure', 'yep', 'nope']
        if text_lower.strip() in simple_responses:
            print(f"[ConversationManager] Pattern: Simple acknowledgment detected")
            return 'none'
        
        # Check for obvious STT error indicators
        stt_error_indicators = [
            '<noise>', '‡∏≠', '‚Ä¶', '...',  # Noise and artifacts from examples
            'vector of', 'book marketing',  # Known STT errors
            'are product', 'there results',
            # Foreign characters mixed with English (common STT issue)
        ]
        
        has_errors = any(indicator in raw_text for indicator in stt_error_indicators)
        
        # Check for very short responses (likely need minimal cleaning)
        if len(raw_text.strip()) < 10:
            print(f"[ConversationManager] Pattern: Very short response")
            return 'light'
        
        # Check for obvious errors or artifacts
        if has_errors or '  ' in raw_text or raw_text.count('.') > 3:
            print(f"[ConversationManager] Pattern: STT errors/artifacts detected")
            return 'full'
        
        # Default to light cleaning for normal conversation
        print(f"[ConversationManager] Pattern: Normal conversation - light cleaning")
        return 'light'
    
    def _simulate_cleaning_process(self, raw_text: str, context: List[Dict], decision: str) -> Dict[str, Any]:
        """
        Simulate the cleaning process (will be replaced with AI in Day 8)
        """
        print(f"[ConversationManager] üõ†Ô∏è Simulating {decision} cleaning process...")
        
        if decision == 'none':
            return {
                'cleaned_text': raw_text,
                'confidence_score': 'HIGH',
                'cleaning_applied': False,
                'corrections': [],
                'context_detected': 'simple_acknowledgment'
            }
        
        # Simulate corrections for known patterns
        cleaned_text = raw_text
        corrections = []
        
        if 'vector of' in raw_text.lower():
            cleaned_text = raw_text.replace('vector of', 'Director of')
            corrections.append({
                'original': 'vector of',
                'corrected': 'Director of',
                'confidence': 'HIGH',
                'reason': 'stt_error_pattern'
            })
        
        if 'book marketing' in raw_text.lower():
            cleaned_text = cleaned_text.replace('book marketing', 'bulk marketing') 
            corrections.append({
                'original': 'book marketing',
                'corrected': 'bulk marketing',
                'confidence': 'MEDIUM',
                'reason': 'contextual_understanding'
            })
        
        cleaning_applied = len(corrections) > 0
        confidence = 'HIGH' if cleaning_applied else 'MEDIUM'
        
        print(f"[ConversationManager] Applied {len(corrections)} corrections")
        for correction in corrections:
            print(f"[ConversationManager] Correction: '{correction['original']}' -> '{correction['corrected']}'")
        
        return {
            'cleaned_text': cleaned_text,
            'confidence_score': confidence,
            'cleaning_applied': cleaning_applied,
            'corrections': corrections,
            'context_detected': 'business_discussion'
        }
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for monitoring"""
        metrics = {}
        
        for metric_name, times in self.performance_metrics.items():
            if times:
                metrics[metric_name] = {
                    'avg_ms': sum(times) / len(times),
                    'max_ms': max(times),
                    'min_ms': min(times),
                    'count': len(times)
                }
            else:
                metrics[metric_name] = {
                    'avg_ms': 0,
                    'max_ms': 0,
                    'min_ms': 0,
                    'count': 0
                }
        
        print(f"[ConversationManager] Performance metrics: {metrics}")
        return metrics